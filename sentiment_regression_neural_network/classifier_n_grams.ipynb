{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Embedding, MaxPooling1D, Flatten, GlobalMaxPool1D, Dropout, Conv1D\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "import keras_metrics\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning the text data for vectorizer\n",
      "loading CountVectorizer\n"
     ]
    }
   ],
   "source": [
    "#from cleaner import Cleaner\n",
    "#cleaner = Cleaner()\n",
    "#cleaner.create_tokenizer_and_clean()\n",
    "\n",
    "from cleaner_n_grams import Cleaner_ngrams\n",
    "cleaner = Cleaner_ngrams()\n",
    "cleaner.create_tokenizer_and_clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(100836,)"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "filename = \"../data/json_bundle_reviews/large-bundle-clean.json\"\n",
    "df = pd.read_json(filename)\n",
    "#df = self.clean_news(df)\n",
    "\n",
    "y = df.sentiment.values\n",
    "sentences = df['content'].values\n",
    "sentences.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
    "sentences, y, test_size=0.25, random_state=1000)\n",
    "maxlen = 250\n",
    "with open('../data/neural_network_config/tokenizer.pickle', 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "\n",
    "X_train =tokenizer.texts_to_sequences(sentences_train)\n",
    "X_test =tokenizer.texts_to_sequences(sentences_test)\n",
    "\n",
    "X_train =pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test =pad_sequences(X_test, padding='post', maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_length = 300\n",
    "\n",
    "        #model = Sequential()\n",
    "#model.add(Embedding(vocab_size, 20, input_length=maxlen))\n",
    "#model.add(Dropout(0.15))\n",
    "#model.add(GlobalMaxPool1D())\n",
    "#model.add(Dense(output_size, activation='sigmoid'))\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 20, input_length=maxlen))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Conv1D(filter_length, 5, activation='relu'))\n",
    "model.add(Conv1D(filter_length, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Conv1D(filter_length, 5, activation='relu'))\n",
    "model.add(Conv1D(filter_length, 5, activation='relu'))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy',keras_metrics.precision(), keras_metrics.recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training model\")\n",
    "csv_logger = CSVLogger('log_loss.csv', append=False, separator=';')\n",
    "\n",
    "callbacks = [\n",
    "ModelCheckpoint(filepath='../data/neural_network_config/temp-model.h5', save_best_only=True), csv_logger]\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=40,\n",
    "                    batch_size=100,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"saving model\")\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"../data/neural_network_config/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"../data/neural_network_config/model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}