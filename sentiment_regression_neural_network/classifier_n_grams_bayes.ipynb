{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Embedding, MaxPooling1D, Flatten, GlobalMaxPool1D, Dropout, Conv1D\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "import keras_metrics\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning the text data for vectorizer\n",
      "(100836, 13)\n",
      "(100267, 13)\n",
      "loading TfidfVectorizer\n"
     ]
    }
   ],
   "source": [
    "#from cleaner import Cleaner\n",
    "#cleaner = Cleaner()\n",
    "#cleaner.create_tokenizer_and_clean()\n",
    "\n",
    "from cleaner_n_grams import Cleaner_ngrams\n",
    "cleaner = Cleaner_ngrams()\n",
    "cleaner.create_tokenizer_and_clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(100267,)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"../data/json_bundle_reviews/large-bundle-clean.json\"\n",
    "df = pd.read_json(filename)\n",
    "\n",
    "y = df.sentiment.values\n",
    "sentences = df['content'].values\n",
    "sentences.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49922\n",
      "50345\n"
     ]
    }
   ],
   "source": [
    "positive_tweets = df[df.sentiment == 1]\n",
    "negative_tweets = df[df.sentiment == 0]\n",
    "\n",
    "print(len(positive_tweets))\n",
    "print(len(negative_tweets))\n",
    "\n",
    "\n",
    "positive_tweets_cutoff = int(len(positive_tweets) * (3./4.))\n",
    "negative_tweets_cutoff = int(len(negative_tweets) * (3./4.))\n",
    "\n",
    "\n",
    "training_tweets = pd.concat([positive_tweets[:positive_tweets_cutoff], negative_tweets[:negative_tweets_cutoff]])\n",
    "test_tweets = pd.concat([positive_tweets[positive_tweets_cutoff:], negative_tweets[negative_tweets_cutoff:]])\n",
    "\n",
    "\n",
    "training_tweets = training_tweets.iloc[np.random.permutation(len(training_tweets))].reset_index(drop=True)\n",
    "test_tweets = test_tweets.iloc[np.random.permutation(len(test_tweets))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "\n",
    "def classify(training_tweets, test_tweets, count_vectorizer):\n",
    "\n",
    "\n",
    "    training_features = count_vectorizer.transform(training_tweets.content.values)\n",
    "    training_labels = training_tweets['sentiment'].values\n",
    "\n",
    "    validation_features = count_vectorizer.transform(test_tweets.content.values)\n",
    "    validation_labels = test_tweets['sentiment'].values\n",
    "\n",
    "    classifier = MultinomialNB()\n",
    "    classifier.fit(training_features, training_labels)\n",
    "    validation_predictions = classifier.predict(validation_features)\n",
    "\n",
    "    confusion = confusion_matrix(validation_labels, validation_predictions)\n",
    "    score = f1_score(validation_labels, validation_predictions)\n",
    "    return classifier,count_vectorizer, score, confusion\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweets classified: 75199\n",
      "0.9787458424596415\n",
      "Confusion matrix:\n",
      "[[12479   108]\n",
      " [  416 12065]]\n",
      "Total tweets classified: 75199\n",
      "0.9739958491026737\n",
      "Confusion matrix:\n",
      "[[12462   125]\n",
      " [  514 11967]]\n",
      "Total tweets classified: 75199\n",
      "0.952092127371829\n",
      "Confusion matrix:\n",
      "[[12283   304]\n",
      " [  865 11616]]\n"
     ]
    }
   ],
   "source": [
    "ngram=(1, 1)\n",
    "count_vectorizer = CountVectorizer(ngram_range=ngram, max_features=3000)\n",
    "count_vectorizer.fit(df.content)\n",
    "\n",
    "classifier,count_vectorizer, score, confusion = classify(training_tweets, test_tweets, count_vectorizer)\n",
    "\n",
    "print ('Total tweets classified: ' + str(len(training_tweets)))\n",
    "print (score)\n",
    "print ('Confusion matrix:')\n",
    "print(confusion)\n",
    "\n",
    "ngram=(1, 2)\n",
    "count_vectorizer = CountVectorizer(ngram_range=ngram,max_features=3000)\n",
    "count_vectorizer.fit(df.content)\n",
    "\n",
    "classifier,count_vectorizer, score, confusion = classify(training_tweets, test_tweets, count_vectorizer)\n",
    "\n",
    "print ('Total tweets classified: ' + str(len(training_tweets)))\n",
    "print (score)\n",
    "print ('Confusion matrix:')\n",
    "print(confusion)\n",
    "\n",
    "\n",
    "ngram=(2, 3)\n",
    "count_vectorizer = CountVectorizer(ngram_range=ngram,max_features=3000)\n",
    "count_vectorizer.fit(df.content)\n",
    "\n",
    "classifier,count_vectorizer, score, confusion = classify(training_tweets, test_tweets, count_vectorizer)\n",
    "\n",
    "print ('Total tweets classified: ' + str(len(training_tweets)))\n",
    "print (score)\n",
    "print ('Confusion matrix:')\n",
    "print(confusion)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweets classified: 75199\n",
      "0.9853498862320865\n",
      "Confusion matrix:\n",
      "[[12359   228]\n",
      " [  139 12342]]\n"
     ]
    }
   ],
   "source": [
    "#chosen model\n",
    "import pickle\n",
    "\n",
    "ngram=(2, 4)\n",
    "count_vectorizer = CountVectorizer(ngram_range=ngram)\n",
    "count_vectorizer.fit(df.content)\n",
    "\n",
    "classifier,count_vectorizer, score, confusion = classify(training_tweets, test_tweets, count_vectorizer)\n",
    "\n",
    "print ('Total tweets classified: ' + str(len(training_tweets)))\n",
    "print (score)\n",
    "print ('Confusion matrix:')\n",
    "print(confusion)\n",
    "\n",
    "import bz2\n",
    "import pickle\n",
    "import _pickle as cPickle\n",
    "\n",
    "def compressed_pickle(title, data):\n",
    "    with bz2.BZ2File(title + '.pbz2', 'w') as f:\n",
    "        cPickle.dump(data, f)\n",
    "\n",
    "def decompress_pickle(file):\n",
    "    data = bz2.BZ2File(file, 'rb')\n",
    "    data = cPickle.load(data)\n",
    "    return data\n",
    "\n",
    "# saving tokenizer\n",
    "with open('../data/neural_network_config/ngram_vectorizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(count_vectorizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "compressed_pickle(\"../data/neural_network_config/ngram_vectorized_compressed\",count_vectorizer)\n",
    "\n",
    "# saving classifier\n",
    "with open('../data/neural_network_config/classifier_naive_bayes.pickle', 'wb') as handle:\n",
    "    pickle.dump(classifier, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "compressed_pickle(\"../data/neural_network_config/classifier_naive_bayes_compressed\",classifier)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "Lo mejor que he visto jamás\n",
      "[1]\n",
      "0.9994635197873718\n",
      "----------------------------------\n",
      "me encanta la tombola\n",
      "[1]\n",
      "0.9846317428608381\n",
      "----------------------------------\n",
      "la tombola es genial, me encanta\n",
      "[1]\n",
      "0.9998457027286963\n",
      "----------------------------------\n",
      "esto es precioso, muy bonito\n",
      "[1]\n",
      "0.9995487078481338\n",
      "----------------------------------\n",
      "me encanta, bello\n",
      "[1]\n",
      "0.8410007407510434\n",
      "----------------------------------\n",
      "lo amo mucho, una preciosidad\n",
      "[1]\n",
      "0.7049016527927292\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------\n",
      "¡Jamás voy a usar esta maldita aplicación!  No funciona para nada.\n",
      "[0]\n",
      "0.00020565692900021603\n",
      "----------------------------------\n",
      "de desagrada profundamente\n",
      "[0]\n",
      "0.3332524917780581\n",
      "----------------------------------\n",
      "no me gusta\n",
      "[0]\n",
      "0.12260075940824233\n",
      "----------------------------------\n",
      "la tombola no es genial, no me gusta\n",
      "[0]\n",
      "0.4862394793527447\n",
      "----------------------------------\n",
      "esto no es precioso no es bonito\n",
      "[1]\n",
      "0.799363174294659\n",
      "----------------------------------\n",
      "todo muy feo y desagradable\n",
      "[0]\n",
      "0.42376571947647096\n",
      "----------------------------------\n",
      "me perece muy triste lo que está ocurriendoque tristeza, qué pena\n",
      "[0]\n",
      "0.0003772228430840919\n"
     ]
    }
   ],
   "source": [
    "#test importation\n",
    "#with open('../data/neural_network_config/ngram_vectorizer.pickle', 'rb') as handle:\n",
    "#    vectorizer = pickle.load(handle)\n",
    "vectorizer = decompress_pickle('../data/neural_network_config/ngram_vectorized_compressed.pbz2')\n",
    "\n",
    "#test importation\n",
    "#with open('../data/neural_network_config/classifier_naive_bayes.pickle', 'rb') as handle:\n",
    "#    classifier = pickle.load(handle)\n",
    "classifier = decompress_pickle('../data/neural_network_config/classifier_naive_bayes_compressed.pbz2')\n",
    "\n",
    "positive_test = [\"Lo mejor que he visto jamás\",\n",
    "                 \"me encanta la tombola\",\n",
    "                 \"la tombola es genial, me encanta\",\n",
    "                 \"esto es precioso, muy bonito\",\n",
    "                 \"me encanta, bello\",\n",
    "                 \"lo amo mucho, una preciosidad\"]\n",
    "\n",
    "negative_test = [\"¡Jamás voy a usar esta maldita aplicación!  No funciona para nada.\",\n",
    "                 \"de desagrada profundamente\",\n",
    "                 \"no me gusta\",\n",
    "                 \"la tombola no es genial, no me gusta\",\n",
    "                 \"esto no es precioso no es bonito\",\n",
    "                 \"todo muy feo y desagradable\",\n",
    "                 \"me perece muy triste lo que está ocurriendo\"\n",
    "                 \"que tristeza, qué pena\"]\n",
    "\n",
    "for text in positive_test:\n",
    "    print(\"----------------------------------\")\n",
    "    print(text)\n",
    "    vals = vectorizer.transform([text])\n",
    "    print(classifier.predict(vals))\n",
    "    print(classifier.predict_proba(vals)[0][1])\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "for text in negative_test:\n",
    "    print(\"----------------------------------\")\n",
    "    print(text)\n",
    "    vals = vectorizer.transform([text])\n",
    "    print(classifier.predict(vals))\n",
    "    print(classifier.predict_proba(vals)[0][1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-cdc073a9",
   "language": "python",
   "display_name": "PyCharm (sentiment-analysis-model-neural-network)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}